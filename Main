import numpy as np
from tensorflow.keras.datasets import mnist

class Neuron:


    def __init__(self,PLConnections, CLConnections, Iid):

        CLw = []     #np.array()
        PLw = []     #np.array()
        CLidl = []
        PLidl = []
        for i in range(CLConnections):
            id = i
            CLw.append(np.random.uniform(-0.8,0.8))
            CLidl.append(id)
        
        for i in range(PLConnections):
            id = i
            PLw.append(np.random.rand())
            PLidl.append(id)

        self.CombinedW = np.concatenate([PLw,CLw])
        self.CombinedIDl = PLidl.copy()
        for i in CLidl:
            self.CombinedIDl.append(i+PLConnections)

        self.id = Iid

        
        pass
    # Desc: Self evident, dont you think

    # Weights is a list of tuples, id (index of the neuron): Weight (weight of the sinapse)
    # weights could include a spcecific set exclusive for inputs and outputs. As it is a "rnn" (maybe) outputs are not alwais useful or
    #  intended by the model, and since its not linear in space maybe we could have a special neuron that chooses when to stop and say this is my answer.   


    # Activation params # todo: chose a better data types, there might be no need for high precision 

    # Modulus = False     # takes the modulo of the activation function, alows for neuron with diferent functions
    # M_scale = 1         # multiplies de derivative of TANH.     f(a*x)
    # B_scale = 0         # translate TANH horizontaly            f(x + b)
    # C_scale = 0         # translate TANH verticaly              f(x) + c
    # Threshold if using 1 bit, like human neurons determines minimum value to round to 1. 


    def Learn():
        
        # will ajust neuron weights acording to some learning function. Could be predictive coding or rat muscle neuron look at Artem Kirsanov

        # if 1 (or not) bit, increase weight or decrease wieghts based activations weighted input alingment with output. 
        # another way would be to have a predictor neuron (predictive coding)

        return 
    
class NeuronMap:

    # This stores the activation level of all neurons at a point in given time.
    # It is used during inferrence of the next time step and might be used during training via 
    # small world network optimization, as pruning of synapses is hevily encuraged for performance 
    # reasons (or not, maybe using a fixed size could be better. real neurons also have a limited
    #  number of connection they can have)


    # Output of the activation of each neuron will be stored acording to some id (index) this id is implicit,
    # a neuron does not know who it is, just who it relates to;

    def __init__(self, nNeuron, PLConnections, CLConnections):
            
        self.FunctionType = 1
        self.NMap = {}
        for i in range(nNeuron):
            self.NMap[i] = Neuron(PLConnections,CLConnections, i)

    def __getitem__(self,key):
        return self.NMap[key]
    
    def __setitem__(self, key, value):
        self.NMap[key] = value

    def copy(self):
        # retorna uma cópia profunda do mapa
        new_map = NeuronMap(0, 0)
        new_map.NMap = self.NMap.copy()
        return new_map
    
    def items(self):
        return self.NMap.items()
    
    def keys(self):
        return self.NMap.keys()
    
    def values(self):
        return self.NMap.values()
    
    def __len__(self):
        return len(self.NMap)
    
    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))
    
    def ReLU(self,x):
        return max(0,x)

    def ActivationFunction(self, neuron, reducedPGmap):

        preActiv = np.array(reducedPGmap) * np.array(neuron.CombinedW)   # PreActiv[i] = PGMap[i] * w[i]

        if self.FunctionType == 1:
            return self.ReLU(np.sum(preActiv))
        if self.FunctionType == 2:
            return self.sigmoid(preActiv)    
        
class GradientMap:

    def __init__(self, nNeuron, nConnections = 0):
            
        self.GMap = {}
        for i in range(nNeuron):
            self.GMap[i] = 0.0

    def __getitem__(self,key):
        return self.GMap[key]

    def __setitem__(self, key, value):
        self.GMap[key] = value
    
    def copy(self):
        # retorna uma cópia profunda do mapa
        new_map = GradientMap(0, 0)
        new_map.GMap = self.GMap.copy()
        return new_map
    
    def items(self):
        return self.GMap.items()
    
    def keys(self):
        return self.GMap.keys()
    
    def values(self):
        return self.GMap.values()
    
    def __len__(self):
        return len(self.GMap)
    
class LayeredRNNModel:

    def __init__(self,FilePath = False):
        
        if(FilePath != False):  # Load Model
            self.LoadModel(FilePath)
        else:    
            #Create New Model
            self.numberOfLayers = 3 # input + Hiden + Output.  min = 2
            self.inputSize = 784
            self.outputSize = 11  # OutputSize + 1 stop neuron

            self.nINeuron = 100
            self.nHNeuron = 50
            self.outputSize = 11 # 10 digits + a "Im done Processing" neuron
            self.nONeuron = 10

            #NeuronMaps are Model Specific
            self.INMap = NeuronMap(self.nINeuron,self.inputSize,self.nINeuron)                 #NeuronMap of input layer
            self.HNMap = NeuronMap(self.nHNeuron,self.nINeuron,self.nHNeuron)                 #NeuronMap of hiden layer
            self.ONMap = NeuronMap(self.nONeuron,self.nHNeuron,self.nONeuron)                 #NeuronMap of output layer
            
            self.NMapList = [self.INMap,self.HNMap,self.ONMap]
            self.BackwardsNMapList = [self.INMap,self.HNMap]

            #Model History for training:
            # list index is the tick and value is a list of [inputMap, inputGradMap, hgradMap, outMap]
            self.History = []

            # ForwardsMap Maps a neuron To a list of neurons that are connected to it and their weights:
            self.LayerForwardsMap = []
            for layer_idx in range(len(self.LayerForwardsMap)):  # entre layer i e layer i+1
                curr_layer = self.NMapList[layer_idx]
                next_layer = self.NMapList[layer_idx + 1]

                forward_map = self.LayerForwardsMap[layer_idx]

                for neuron in next_layer.NMap.values():  # cada neurônio da próxima camada
                    for idx, input_id in enumerate(neuron.CombinedIDl):
                        weight = neuron.CombinedW[idx]
                        if input_id not in forward_map:
                            forward_map[input_id] = []
                        forward_map[input_id].append({neuron.id: weight})
            


    def TickTimeLayer(self, NMap, PLGradMap, PastCLGradMap, CLGradMap):

        # Combina os mapas PL + CL
        CombGradMap = {}
        for id in PLGradMap.keys():
            CombGradMap[id] = PLGradMap[id]

        for id in PastCLGradMap.keys():
            CombGradMap[id + len(PLGradMap)] = PastCLGradMap[id]

        for id, neuron in NMap.NMap.items():
            reducedCombGradMap = [CombGradMap[i] for i in neuron.CombinedIDl]
            CLGradMap[id] = NMap.ActivationFunction(neuron, reducedCombGradMap)/len(NMap)
        return "placehodler"

    def LocalBP(self, Layer, Neuron, PLGradMap, CLGradMap, NLGradmap):

        AverageWeightedDependentActivation = 0
        TotalWeight = 0
        for IdWPair in self.LayerForwardsMap[Layer][Neuron.ID]:
            for ID in IdWPair.keys():
                AverageWeightedDependentActivation += NLGradmap.GMap[ID] * IdWPair[ID]
                TotalWeight += IdWPair[ID]

        AverageWeightedDependentActivation /= TotalWeight  # TODO Consider cliping weights to [-1,1] or smh
        
        Error = AverageWeightedDependentActivation - CLGradMap[Neuron.id]




        return
    
    def Inference(self,inferenceSteps,INPUT  = False, LOG = True):

         # GradMaps are inference Specific
        inputMap = GradientMap(self.inputSize)           #GradMap of the inputs
        inputGradMap = GradientMap(self.nINeuron)       #GradMap of the input layer
        hgradMap = GradientMap(self.nHNeuron)           #GradMap of the hiden Layer
        outMap = GradientMap(self.nONeuron)             #GradMap of the output layer

        if LOG:
            a = []
            for i in self.INMap.NMap:
                a.append(i)

            b = []
            for i in self.HNMap.NMap:
                b.append(i)
    
            c = []
            for i in self.ONMap.NMap:
                c.append(i)

            print(a)
            Nlist = (self.INMap.NMap.values()) # list of {id:neuron}
            print(type(Nlist))
            alist = []
            for i in Nlist:
                alist.append(i)
            print(alist[0].CombinedIDl)
            print(alist[0].CombinedW)
            print(b)
            print(c)

        image_pixels = []
        INPUT = False
        # if input == False:
        for i in range(self.inputSize):
            inputMap[i] = np.random.rand()    #image_pixels[i]
        #else:
        #    for i in range(len(INPUT)):
        #        inputMap[i] = INPUT[i]

    #   uses a theread pool for the neurons and layers
        for i in range(inferenceSteps):
        # update inputmap, if input changed
            PinputGradMap = inputGradMap.copy()
            PhgradMap = hgradMap.copy()
            PoutMap = outMap.copy() 

        # Multitreading Logic{


        # Importatnt: TickTime Funcition can be made to edit inputGradMap directly instead of returning a value to be copied. The P...Maps alow for this. TODO
            self.TickTimeLayer(self.INMap, inputMap, PinputGradMap, inputGradMap)

            self.TickTimeLayer(self.HNMap, PinputGradMap, PhgradMap, hgradMap)

            self.TickTimeLayer(self.ONMap, PhgradMap, PoutMap, outMap)

        #}
    
            print("Step", i)
            print("inputMap",inputMap.GMap.values())
            print("inputGradMap:", inputGradMap.GMap.values())
            print("hgradMap:", hgradMap.GMap.values())
            print("outMap:", outMap.GMap.values())
        # save GMaps if needed for training

            self.History.append([inputMap, inputGradMap, hgradMap, outMap])

        return outMap
    

    def SaveModel():
        return      
    def LoadModel(filepath):
        return

    def Train(DATASET):

        inferenceSteps = 1
        #DATASET = (DATA, LABEL)
        for DATA in range(len(DATASET[0])):
            output = Model.Inference(inferenceSteps, DATA)
            inputMap, inputGradMap, hgradMap, outMap = Model.History[-1]






        # LoadDATASET
        # for i in range len(DATA)
        # Inference on DATA[i]
        # BP, Hebbian, whatever
        # 
        #    every 100 or 1000, save model on disk
        return
    
def load_mnist_subset(n_samples=1000):
    (x_train, y_train), (_, _) = mnist.load_data()
    
    # Reduz para um subconjunto menor para testes rápidos
    x_train = x_train[:n_samples]
    y_train = y_train[:n_samples]
    
    # Normaliza para [0,1] e achata para vetor 784 (28x28)
    x_train = x_train.astype(np.float32) / 255.0
    x_train = x_train.reshape((n_samples, 784))

    # One-hot encode para targets (0–9 → [0 0 0 1 0 ...])
    y_encoded = np.zeros((n_samples, 10), dtype=np.float32)
    for i, label in enumerate(y_train):
        y_encoded[i, label] = 1.0

    return x_train, y_encoded





Model = LayeredRNNModel()
inferenceSteps = 10
output = Model.Inference(inferenceSteps)
print(output.values())
print(len(Model.History))
